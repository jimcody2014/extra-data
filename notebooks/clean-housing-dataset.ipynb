{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Housing Dataset\n",
    "\n",
    "In this lab, you'll learn how to clean real-world housing data using Pandas. You'll practice fixing common issues like missing values, outliers, inconsistent text, and logical errors. Data cleaning is a key step to make sure your analysis is accurate and trustworthy.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Identify and assess common data quality issues in real-world datasets\n",
    "- Standardize text fields and categorical variables for consistency\n",
    "- Handle missing values using appropriate imputation strategies\n",
    "- Detect and correct outliers and impossible values\n",
    "- Remove duplicate records effectively\n",
    "- Validate cleaned data to ensure quality standards\n",
    "- Document data cleaning processes for reproducibility and transparency\n",
    "\n",
    "## Why Data Cleaning Matters\n",
    "Real-world data is almost never clean when you first receive it. Understanding how to systematically identify and fix data quality issues is one of the most valuable skills in data science. Poor data quality can lead to incorrect conclusions, failed models, and bad business decisions.\n",
    "\n",
    "This lab simulates the kinds of data quality problems you'll encounter in professional settings, where data comes from multiple sources, different systems, and various people with different standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding the Messy Dataset\n",
    "\n",
    "Real-world data is often messy. Errors, missing values, and inconsistencies are common due to manual entry, system differences, and changing standards. In this lab, you'll work with a housing dataset that includes these typical problems and learn how to clean them step by step.\n",
    "\n",
    "**Key Insight**: The messiness in this dataset isn't random‚Äîit follows patterns you'll see in real business data. Understanding these patterns helps you clean data more efficiently and systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üè† Creating a realistic 'messy' housing dataset...\")\n",
    "print(\"This simulates data that might come from multiple sources with different standards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base data with realistic variations\n",
    "n_properties = 600\n",
    "property_ids = list(range(1001, 1001 + n_properties))\n",
    "\n",
    "# Create realistic property characteristics\n",
    "square_footage = np.random.normal(2000, 500, n_properties).astype(int)\n",
    "square_footage = np.clip(square_footage, 600, 5000)\n",
    "\n",
    "bedrooms = np.random.choice([1, 2, 3, 4, 5, 6], n_properties, p=[0.05, 0.25, 0.35, 0.25, 0.08, 0.02])\n",
    "bathrooms = np.random.normal(2.5, 0.8, n_properties)\n",
    "bathrooms = np.clip(bathrooms, 1, 5)\n",
    "\n",
    "ages = np.random.exponential(15, n_properties).astype(int)\n",
    "ages = np.clip(ages, 0, 120)\n",
    "\n",
    "print(f\"‚úì Generated base characteristics for {n_properties} properties\")\n",
    "print(f\"  Square footage range: {square_footage.min():,} - {square_footage.max():,} sq ft\")\n",
    "print(f\"  Bedroom range: {bedrooms.min()} - {bedrooms.max()}\")\n",
    "print(f\"  Age range: {ages.min()} - {ages.max()} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create property types with inconsistent formatting\n",
    "property_types_messy = [\n",
    "    'Single Family', 'single family', 'SINGLE FAMILY', 'Single-Family', 'SF', 'SFH',\n",
    "    'Townhouse', 'townhouse', 'TOWNHOUSE', 'Town House', 'TH', \n",
    "    'Condo', 'condo', 'CONDO', 'Condominium', 'Apartment', 'apt',\n",
    "    'Duplex', 'duplex', 'DUPLEX', '2-unit', 'Two Unit'\n",
    "]\n",
    "\n",
    "# Normalize probabilities so they sum to 1\n",
    "property_types_probs = np.array([0.15, 0.1, 0.05, 0.08, 0.1, 0.02, \n",
    "                                0.08, 0.05, 0.03, 0.02, 0.05,\n",
    "                                0.08, 0.05, 0.02, 0.04, 0.02, 0.02,\n",
    "                                0.03, 0.02, 0.01, 0.02, 0.01])\n",
    "property_types_probs = property_types_probs / property_types_probs.sum()\n",
    "\n",
    "property_types = np.random.choice(property_types_messy, n_properties, \n",
    "                                 p=property_types_probs)\n",
    "\n",
    "# Create neighborhoods with spacing and capitalization issues\n",
    "neighborhoods_messy = [\n",
    "    'Downtown', 'downtown', ' Downtown ', 'Down Town', 'DOWNTOWN',\n",
    "    'Suburban', 'suburban', 'SUBURBAN', ' Suburban', 'Suburban ',\n",
    "    'Riverside', 'riverside', 'RIVERSIDE', 'River Side', ' Riverside ',\n",
    "    'Historic District', 'historic district', 'HISTORIC DISTRICT', \n",
    "    'Historic Dist', 'Historic_District', ' Historic District ',\n",
    "    'New Development', 'new development', 'NEW DEVELOPMENT', \n",
    "    'New Dev', 'NewDevelopment', ' New Development '\n",
    "]\n",
    "\n",
    "# Normalize neighborhood probabilities so they sum to 1\n",
    "neighborhoods_probs = np.array([\n",
    "    0.08, 0.05, 0.02, 0.02, 0.03,\n",
    "    0.15, 0.1, 0.05, 0.02, 0.03,\n",
    "    0.1, 0.05, 0.02, 0.02, 0.01,\n",
    "    0.08, 0.03, 0.02, 0.02, 0.01, 0.02,\n",
    "    0.1, 0.05, 0.03, 0.02, 0.01, 0.02\n",
    "])\n",
    "neighborhoods_probs = neighborhoods_probs / neighborhoods_probs.sum()\n",
    "\n",
    "neighborhoods = np.random.choice(neighborhoods_messy, n_properties,\n",
    "                                p=neighborhoods_probs)\n",
    "\n",
    "print(f\"\\n‚úì Created inconsistent categorical data\")\n",
    "print(f\"  Property type variations: {len(np.unique(property_types))}\")\n",
    "print(f\"  Neighborhood variations: {len(np.unique(neighborhoods))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate realistic prices\n",
    "base_price = 120 * square_footage\n",
    "bedroom_bonus = bedrooms * 12000\n",
    "bathroom_bonus = bathrooms * 8000\n",
    "age_penalty = ages * 800\n",
    "\n",
    "prices = base_price + bedroom_bonus + bathroom_bonus - age_penalty\n",
    "prices = prices * np.random.normal(1.0, 0.2, n_properties)\n",
    "prices = np.round(prices, -3).astype(int)\n",
    "\n",
    "# Create initial DataFrame\n",
    "housing_data = pd.DataFrame({\n",
    "    'property_id': property_ids,\n",
    "    'square_feet': square_footage,\n",
    "    'bedrooms': bedrooms,\n",
    "    'bathrooms': bathrooms,\n",
    "    'age_years': ages,\n",
    "    'property_type': property_types,\n",
    "    'neighborhood': neighborhoods,\n",
    "    'price': prices\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úì Created initial housing dataset\")\n",
    "print(f\"  Shape: {housing_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(housing_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce realistic data quality issues\n",
    "print(\"\\nüîß Introducing realistic data quality issues...\")\n",
    "\n",
    "# Missing values\n",
    "missing_price_indices = np.random.choice(housing_data.index, size=25, replace=False)\n",
    "housing_data.loc[missing_price_indices, 'price'] = np.nan\n",
    "\n",
    "missing_bathroom_indices = np.random.choice(housing_data.index, size=15, replace=False)\n",
    "housing_data.loc[missing_bathroom_indices, 'bathrooms'] = np.nan\n",
    "\n",
    "missing_age_indices = np.random.choice(housing_data.index, size=20, replace=False)\n",
    "housing_data.loc[missing_age_indices, 'age_years'] = np.nan\n",
    "\n",
    "# Duplicates\n",
    "duplicate_indices = np.random.choice(housing_data.index[:-10], size=5, replace=False)\n",
    "for idx in duplicate_indices:\n",
    "    housing_data.loc[len(housing_data)] = housing_data.loc[idx].copy()\n",
    "\n",
    "# Impossible values\n",
    "housing_data.loc[np.random.choice(housing_data.index, 3), 'age_years'] = [-5, -2, -10]\n",
    "housing_data.loc[np.random.choice(housing_data.index, 2), 'bathrooms'] = [0, -1]\n",
    "housing_data.loc[np.random.choice(housing_data.index, 3), 'price'] = [50000, 5000000, 25000]\n",
    "\n",
    "# Inconsistent property IDs\n",
    "id_issues = np.random.choice(housing_data.index, 8, replace=False)\n",
    "housing_data.loc[id_issues[:3], 'property_id'] = ['ID1001', 'prop_1002', 'P1003']\n",
    "housing_data.loc[id_issues[3:6], 'property_id'] = [None, '', 'MISSING']\n",
    "\n",
    "print(f\"‚úì Data quality issues introduced:\")\n",
    "print(f\"  ‚Ä¢ Missing values in multiple columns\")\n",
    "print(f\"  ‚Ä¢ Duplicate records\")\n",
    "print(f\"  ‚Ä¢ Impossible values (negative ages, zero bathrooms)\")\n",
    "print(f\"  ‚Ä¢ Extreme outliers\")\n",
    "print(f\"  ‚Ä¢ Inconsistent text formatting\")\n",
    "print(f\"\\nüìä Final messy dataset: {len(housing_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1:** Explore the messy dataset:\n",
    "1. Examine the first 10 rows and identify obvious problems\n",
    "2. Check unique values in categorical columns\n",
    "3. Look at data types and basic statistics\n",
    "4. Count missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exploration code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Initial Data Quality Assessment\n",
    "\n",
    "Before cleaning data, it's important to identify and understand all data quality issues. This assessment helps you plan your cleaning steps, prioritize problems, and estimate the effort needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPREHENSIVE DATA QUALITY ASSESSMENT ===\")\n",
    "print(f\"Dataset shape: {housing_data.shape}\")\n",
    "\n",
    "# Missing value analysis\n",
    "print(\"\\n1. MISSING VALUES ANALYSIS\")\n",
    "print(\"-\" * 30)\n",
    "missing_summary = housing_data.isnull().sum()\n",
    "missing_percentages = (missing_summary / len(housing_data)) * 100\n",
    "\n",
    "for column in housing_data.columns:\n",
    "    missing_count = missing_summary[column]\n",
    "    missing_pct = missing_percentages[column]\n",
    "    status = \"‚úì Clean\" if missing_count == 0 else f\"‚ö† {missing_count} missing ({missing_pct:.1f}%)\"\n",
    "    print(f\"{column:15} {status}\")\n",
    "\n",
    "print(f\"\\nTotal missing values: {missing_summary.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate analysis\n",
    "print(\"\\n2. DUPLICATE RECORDS ANALYSIS\")\n",
    "print(\"-\" * 30)\n",
    "total_duplicates = housing_data.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {total_duplicates}\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    print(\"\\nSample duplicate records:\")\n",
    "    duplicate_rows = housing_data[housing_data.duplicated(keep=False)].sort_values('property_id')\n",
    "    print(duplicate_rows[['property_id', 'square_feet', 'bedrooms', 'price']].head())\n",
    "\n",
    "# Text formatting issues\n",
    "print(\"\\n3. TEXT FORMATTING INCONSISTENCIES\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "print(f\"Property type variations: {housing_data['property_type'].nunique()}\")\n",
    "print(f\"Sample variations: {list(housing_data['property_type'].unique())[:8]}\")\n",
    "\n",
    "print(f\"\\nNeighborhood variations: {housing_data['neighborhood'].nunique()}\")\n",
    "print(f\"Sample variations: {list(housing_data['neighborhood'].unique())[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for impossible values\n",
    "print(\"\\n4. IMPOSSIBLE VALUES DETECTION\")\n",
    "print(\"-\" * 31)\n",
    "\n",
    "negative_ages = (housing_data['age_years'] < 0).sum()\n",
    "print(f\"Properties with negative age: {negative_ages}\")\n",
    "if negative_ages > 0:\n",
    "    print(f\"  Values: {housing_data[housing_data['age_years'] < 0]['age_years'].tolist()}\")\n",
    "\n",
    "impossible_bathrooms = (housing_data['bathrooms'] <= 0).sum()\n",
    "print(f\"Properties with ‚â§0 bathrooms: {impossible_bathrooms}\")\n",
    "\n",
    "# Price outliers using IQR method\n",
    "if housing_data['price'].notna().sum() > 0:\n",
    "    Q1 = housing_data['price'].quantile(0.25)\n",
    "    Q3 = housing_data['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    price_outliers = ((housing_data['price'] < Q1 - 3*IQR) | \n",
    "                     (housing_data['price'] > Q3 + 3*IQR)).sum()\n",
    "    print(f\"Extreme price outliers: {price_outliers}\")\n",
    "\n",
    "print(f\"\\nüìã Assessment complete - roadmap for cleaning established!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1:** Based on the assessment, prioritize the issues:\n",
    "1. Which problems should be fixed first and why?\n",
    "2. What additional information would help you decide how to handle missing values?\n",
    "3. How might the severity of these issues depend on your analysis goals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prioritization analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Cleaning Text Data and Standardizing Categories\n",
    "\n",
    "Text columns often have inconsistent formatting in real-world data. Standardizing categorical variables is important for accurate analysis. The goal is to automate as much as possible while avoiding losing important differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy\n",
    "housing_clean = housing_data.copy()\n",
    "\n",
    "print(\"=== CLEANING TEXT DATA AND STANDARDIZING CATEGORIES ===\")\n",
    "print(\"üîß Creating working copy of data for cleaning...\")\n",
    "\n",
    "def standardize_property_type(prop_type):\n",
    "    \"\"\"\n",
    "    Standardize property type values to consistent categories.\n",
    "    Uses rule-based approach to handle variations in capitalization,\n",
    "    spacing, and abbreviations.\n",
    "    \"\"\"\n",
    "    if pd.isna(prop_type):\n",
    "        return prop_type\n",
    "    \n",
    "    prop_type_clean = str(prop_type).lower().strip()\n",
    "    \n",
    "    if any(term in prop_type_clean for term in ['single', 'sf', 'sfh']):\n",
    "        return 'Single Family'\n",
    "    elif any(term in prop_type_clean for term in ['town', 'th']):\n",
    "        return 'Townhouse'\n",
    "    elif any(term in prop_type_clean for term in ['condo', 'apartment', 'apt']):\n",
    "        return 'Condo'\n",
    "    elif any(term in prop_type_clean for term in ['duplex', '2-unit', 'two unit']):\n",
    "        return 'Duplex'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Test the function\n",
    "test_types = ['Single Family', 'single family', 'SF', 'TOWNHOUSE', 'apt']\n",
    "print(\"\\nTesting property type standardization:\")\n",
    "for test_type in test_types:\n",
    "    result = standardize_property_type(test_type)\n",
    "    print(f\"  '{test_type}' ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_neighborhood(neighborhood):\n",
    "    \"\"\"\n",
    "    Standardize neighborhood values to consistent categories.\n",
    "    Handles spacing, capitalization, and alternative spellings.\n",
    "    \"\"\"\n",
    "    if pd.isna(neighborhood):\n",
    "        return neighborhood\n",
    "    \n",
    "    neighborhood_clean = str(neighborhood).lower().strip().replace('_', ' ')\n",
    "    \n",
    "    if 'downtown' in neighborhood_clean or 'down town' in neighborhood_clean:\n",
    "        return 'Downtown'\n",
    "    elif 'suburban' in neighborhood_clean:\n",
    "        return 'Suburban'\n",
    "    elif 'riverside' in neighborhood_clean or 'river side' in neighborhood_clean:\n",
    "        return 'Riverside'\n",
    "    elif 'historic' in neighborhood_clean:\n",
    "        return 'Historic District'\n",
    "    elif 'new' in neighborhood_clean and ('dev' in neighborhood_clean or \n",
    "                                         'development' in neighborhood_clean):\n",
    "        return 'New Development'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def clean_property_id(prop_id):\n",
    "    \"\"\"\n",
    "    Extract numeric property ID from various formats.\n",
    "    Handles mixed formats by extracting the numeric portion.\n",
    "    \"\"\"\n",
    "    if pd.isna(prop_id) or prop_id == '' or str(prop_id).upper() == 'MISSING':\n",
    "        return np.nan\n",
    "    \n",
    "    prop_id_str = str(prop_id)\n",
    "    digits = re.findall(r'\\d+', prop_id_str)\n",
    "    \n",
    "    if digits:\n",
    "        return int(digits[0])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply standardization functions\n",
    "print(\"\\nApplying standardization functions...\")\n",
    "print(f\"Before: {housing_clean['property_type'].nunique()} property types, {housing_clean['neighborhood'].nunique()} neighborhoods\")\n",
    "\n",
    "housing_clean['property_type'] = housing_clean['property_type'].apply(standardize_property_type)\n",
    "housing_clean['neighborhood'] = housing_clean['neighborhood'].apply(standardize_neighborhood)\n",
    "housing_clean['property_id'] = housing_clean['property_id'].apply(clean_property_id)\n",
    "\n",
    "print(f\"After: {housing_clean['property_type'].nunique()} property types, {housing_clean['neighborhood'].nunique()} neighborhoods\")\n",
    "print(\"\\n‚úÖ Text standardization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1:** Practice creating standardization functions for other data types like phone numbers, email domains, or state abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your standardization practice here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Handling Missing Values\n",
    "\n",
    "Missing values must be handled carefully because how you fill them can impact your analysis. Choose an imputation method that fits your data and why values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# Strategy 1: Remove rows with missing property IDs (essential identifiers)\n",
    "print(\"\\n1. HANDLING MISSING PROPERTY IDs\")\n",
    "print(\"Strategy: Remove records (IDs are essential identifiers)\")\n",
    "\n",
    "before_removal = len(housing_clean)\n",
    "housing_clean = housing_clean.dropna(subset=['property_id'])\n",
    "after_removal = len(housing_clean)\n",
    "removed_count = before_removal - after_removal\n",
    "\n",
    "print(f\"Removed {removed_count} rows with missing property IDs\")\n",
    "print(f\"Remaining records: {after_removal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Impute missing ages using neighborhood patterns\n",
    "print(\"\\n2. HANDLING MISSING AGE VALUES\")\n",
    "print(\"Strategy: Use neighborhood medians (similar areas built at similar times)\")\n",
    "\n",
    "missing_age_count = housing_clean['age_years'].isna().sum()\n",
    "print(f\"Properties with missing age: {missing_age_count}\")\n",
    "\n",
    "if missing_age_count > 0:\n",
    "    for neighborhood in housing_clean['neighborhood'].unique():\n",
    "        if pd.notna(neighborhood):\n",
    "            neighborhood_mask = housing_clean['neighborhood'] == neighborhood\n",
    "            neighborhood_ages = housing_clean.loc[neighborhood_mask, 'age_years']\n",
    "            \n",
    "            if neighborhood_ages.notna().sum() > 0:\n",
    "                median_age = neighborhood_ages.median()\n",
    "                missing_in_neighborhood = neighborhood_ages.isna().sum()\n",
    "                \n",
    "                if missing_in_neighborhood > 0:\n",
    "                    housing_clean.loc[neighborhood_mask & housing_clean['age_years'].isna(), 'age_years'] = median_age\n",
    "                    print(f\"  {neighborhood}: filled {missing_in_neighborhood} values with {median_age:.0f} years\")\n",
    "    \n",
    "    # Fill any remaining missing values with overall median\n",
    "    remaining_missing = housing_clean['age_years'].isna().sum()\n",
    "    if remaining_missing > 0:\n",
    "        overall_median = housing_clean['age_years'].median()\n",
    "        housing_clean['age_years'].fillna(overall_median, inplace=True)\n",
    "        print(f\"  Filled remaining {remaining_missing} values with overall median: {overall_median:.0f} years\")\n",
    "\n",
    "print(\"‚úÖ Age imputation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Impute bathrooms based on bedrooms and property type\n",
    "print(\"\\n3. HANDLING MISSING BATHROOM VALUES\")\n",
    "print(\"Strategy: Use bedroom and property type patterns\")\n",
    "\n",
    "missing_bathroom_count = housing_clean['bathrooms'].isna().sum()\n",
    "print(f\"Properties with missing bathrooms: {missing_bathroom_count}\")\n",
    "\n",
    "if missing_bathroom_count > 0:\n",
    "    for prop_type in housing_clean['property_type'].unique():\n",
    "        if pd.notna(prop_type):\n",
    "            for bedrooms in sorted(housing_clean['bedrooms'].unique()):\n",
    "                if pd.notna(bedrooms):\n",
    "                    combined_mask = ((housing_clean['property_type'] == prop_type) & \n",
    "                                   (housing_clean['bedrooms'] == bedrooms))\n",
    "                    reference_bathrooms = housing_clean.loc[combined_mask, 'bathrooms']\n",
    "                    \n",
    "                    if reference_bathrooms.notna().sum() >= 3:\n",
    "                        median_bathrooms = reference_bathrooms.median()\n",
    "                        missing_mask = combined_mask & housing_clean['bathrooms'].isna()\n",
    "                        missing_count = missing_mask.sum()\n",
    "                        \n",
    "                        if missing_count > 0:\n",
    "                            housing_clean.loc[missing_mask, 'bathrooms'] = median_bathrooms\n",
    "                            print(f\"  {prop_type}, {bedrooms} bed: filled {missing_count} values with {median_bathrooms:.1f} bathrooms\")\n",
    "    \n",
    "    # Apply fallback rule for remaining missing values\n",
    "    remaining_missing = housing_clean['bathrooms'].isna().sum()\n",
    "    if remaining_missing > 0:\n",
    "        for idx in housing_clean[housing_clean['bathrooms'].isna()].index:\n",
    "            bedrooms = housing_clean.loc[idx, 'bedrooms']\n",
    "            estimated_bathrooms = max(1.0, bedrooms * 0.75 + 0.5)\n",
    "            housing_clean.loc[idx, 'bathrooms'] = round(estimated_bathrooms * 2) / 2\n",
    "        print(f\"  Applied bedroom-based estimation for remaining {remaining_missing} values\")\n",
    "\n",
    "print(\"‚úÖ Bathroom imputation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Impute prices using similar properties\n",
    "print(\"\\n4. HANDLING MISSING PRICE VALUES\")\n",
    "print(\"Strategy: Find similar properties and use their median price\")\n",
    "\n",
    "missing_price_count = housing_clean['price'].isna().sum()\n",
    "print(f\"Properties with missing prices: {missing_price_count}\")\n",
    "\n",
    "if missing_price_count > 0:\n",
    "    has_price = housing_clean['price'].notna()\n",
    "    imputation_count = 0\n",
    "    \n",
    "    for idx in housing_clean[housing_clean['price'].isna()].index:\n",
    "        current_property = housing_clean.loc[idx]\n",
    "        \n",
    "        # Find similar properties (size, bedrooms, type)\n",
    "        similar_mask = (\n",
    "            (abs(housing_clean['square_feet'] - current_property['square_feet']) <= 200) &\n",
    "            (housing_clean['bedrooms'] == current_property['bedrooms']) &\n",
    "            (housing_clean['property_type'] == current_property['property_type']) &\n",
    "            has_price\n",
    "        )\n",
    "        \n",
    "        similar_properties = housing_clean[similar_mask]\n",
    "        \n",
    "        if len(similar_properties) >= 3:\n",
    "            estimated_price = similar_properties['price'].median()\n",
    "            housing_clean.loc[idx, 'price'] = estimated_price\n",
    "            imputation_count += 1\n",
    "            if imputation_count <= 3:\n",
    "                print(f\"  Property {int(current_property['property_id'])}: ${estimated_price:,.0f} (based on {len(similar_properties)} similar properties)\")\n",
    "        else:\n",
    "            # Fallback: neighborhood and property type median\n",
    "            fallback_mask = (\n",
    "                (housing_clean['neighborhood'] == current_property['neighborhood']) &\n",
    "                (housing_clean['property_type'] == current_property['property_type']) &\n",
    "                has_price\n",
    "            )\n",
    "            fallback_properties = housing_clean[fallback_mask]\n",
    "            \n",
    "            if len(fallback_properties) > 0:\n",
    "                estimated_price = fallback_properties['price'].median()\n",
    "                housing_clean.loc[idx, 'price'] = estimated_price\n",
    "                imputation_count += 1\n",
    "    \n",
    "    if imputation_count > 3:\n",
    "        print(f\"  ... and {imputation_count - 3} other imputations\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully imputed {imputation_count} missing prices\")\n",
    "\n",
    "# Final check\n",
    "final_missing = housing_clean.isnull().sum().sum()\n",
    "print(f\"\\nüìä Final missing values: {final_missing}\")\n",
    "if final_missing == 0:\n",
    "    print(\"üéâ All missing values successfully handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1:** Evaluate the missing value strategies and consider alternatives for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your missing value evaluation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Removing Duplicates and Handling Outliers\n",
    "\n",
    "Duplicates and outliers can distort your analysis. We need to distinguish between errors (clearly wrong values) and legitimate extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== REMOVING DUPLICATES ===\")\n",
    "\n",
    "duplicates_before = housing_clean.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    housing_clean = housing_clean.drop_duplicates()\n",
    "    duplicates_after = housing_clean.duplicated().sum()\n",
    "    removed = duplicates_before - duplicates_after\n",
    "    print(f\"‚úÖ Removed {removed} duplicate records\")\n",
    "\n",
    "# Check for duplicate property IDs\n",
    "id_duplicates = housing_clean['property_id'].duplicated().sum()\n",
    "if id_duplicates > 0:\n",
    "    housing_clean = housing_clean.drop_duplicates(subset=['property_id'])\n",
    "    print(f\"‚úÖ Removed {id_duplicates} duplicate property IDs\")\n",
    "\n",
    "print(f\"Final dataset shape: {housing_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== HANDLING IMPOSSIBLE VALUES ===\")\n",
    "\n",
    "# Fix negative ages\n",
    "negative_ages = (housing_clean['age_years'] < 0).sum()\n",
    "if negative_ages > 0:\n",
    "    print(f\"Found {negative_ages} negative ages - converting to positive\")\n",
    "    housing_clean.loc[housing_clean['age_years'] < 0, 'age_years'] = \\\n",
    "        abs(housing_clean.loc[housing_clean['age_years'] < 0, 'age_years'])\n",
    "    print(\"‚úÖ Negative ages corrected\")\n",
    "\n",
    "# Fix impossible bathroom counts\n",
    "impossible_bathrooms = (housing_clean['bathrooms'] <= 0).sum()\n",
    "if impossible_bathrooms > 0:\n",
    "    print(f\"Found {impossible_bathrooms} properties with ‚â§0 bathrooms - setting to 1.0\")\n",
    "    housing_clean.loc[housing_clean['bathrooms'] <= 0, 'bathrooms'] = 1.0\n",
    "    print(\"‚úÖ Impossible bathroom counts corrected\")\n",
    "\n",
    "if negative_ages == 0 and impossible_bathrooms == 0:\n",
    "    print(\"‚úÖ No impossible values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== HANDLING EXTREME OUTLIERS ===\")\n",
    "\n",
    "def detect_outliers_iqr(data, column, factor=3):\n",
    "    \"\"\"Detect outliers using IQR method with specified factor.\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    \n",
    "    outlier_mask = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    \n",
    "    return {\n",
    "        'count': outlier_mask.sum(),\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'mask': outlier_mask\n",
    "    }\n",
    "\n",
    "# Analyze price outliers\n",
    "price_outliers = detect_outliers_iqr(housing_clean, 'price', factor=3)\n",
    "print(f\"Price outliers found: {price_outliers['count']}\")\n",
    "print(f\"Normal range: ${price_outliers['lower_bound']:,.0f} - ${price_outliers['upper_bound']:,.0f}\")\n",
    "\n",
    "if price_outliers['count'] > 0:\n",
    "    extreme_outliers = housing_clean.loc[price_outliers['mask']]\n",
    "    corrections_made = 0\n",
    "    \n",
    "    for idx, row in extreme_outliers.iterrows():\n",
    "        price = row['price']\n",
    "        \n",
    "        if price < 100000:  # Very low prices - likely errors\n",
    "            # Find similar properties for correction\n",
    "            similar_properties = housing_clean[\n",
    "                (housing_clean['square_feet'].between(row['square_feet'] - 300, row['square_feet'] + 300)) &\n",
    "                (housing_clean['neighborhood'] == row['neighborhood']) &\n",
    "                (housing_clean['property_type'] == row['property_type']) &\n",
    "                (~price_outliers['mask'])\n",
    "            ]\n",
    "            \n",
    "            if len(similar_properties) > 0:\n",
    "                corrected_price = similar_properties['price'].median()\n",
    "                housing_clean.loc[idx, 'price'] = corrected_price\n",
    "                corrections_made += 1\n",
    "                if corrections_made <= 3:\n",
    "                    print(f\"  Corrected ${price:,} ‚Üí ${corrected_price:,.0f} (Property {int(row['property_id'])})\")\n",
    "    \n",
    "    if corrections_made > 3:\n",
    "        print(f\"  ... and {corrections_made - 3} other corrections\")\n",
    "    \n",
    "    print(f\"‚úÖ Corrected {corrections_made} obvious price errors\")\n",
    "else:\n",
    "    print(\"‚úÖ No extreme price outliers found\")\n",
    "\n",
    "print(\"\\n‚úÖ Outlier handling completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.1:** Practice outlier detection with different IQR factors and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your outlier analysis practice here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Validating the Cleaned Dataset\n",
    "\n",
    "After cleaning, validate that your dataset is accurate and ready for analysis. Think of validation as final quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VALIDATING THE CLEANED DATASET ===\")\n",
    "\n",
    "print(f\"üìä Final dataset shape: {housing_clean.shape}\")\n",
    "print(f\"üìä Data retention rate: {(len(housing_clean) / len(housing_data)) * 100:.1f}%\")\n",
    "\n",
    "# Check for remaining issues\n",
    "remaining_missing = housing_clean.isnull().sum().sum()\n",
    "remaining_duplicates = housing_clean.duplicated().sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Remaining missing values: {remaining_missing}\")\n",
    "print(f\"‚úÖ Remaining duplicates: {remaining_duplicates}\")\n",
    "\n",
    "# Validate value ranges\n",
    "print(\"\\nüìè VALUE RANGE VALIDATION:\")\n",
    "validation_rules = {\n",
    "    'property_id': {'min': 1000, 'max': 2000},\n",
    "    'square_feet': {'min': 400, 'max': 5000},\n",
    "    'bedrooms': {'min': 1, 'max': 6},\n",
    "    'bathrooms': {'min': 0.5, 'max': 5},\n",
    "    'age_years': {'min': 0, 'max': 150},\n",
    "    'price': {'min': 50000, 'max': 3000000}\n",
    "}\n",
    "\n",
    "all_ranges_valid = True\n",
    "for column, rules in validation_rules.items():\n",
    "    min_val = housing_clean[column].min()\n",
    "    max_val = housing_clean[column].max()\n",
    "    \n",
    "    min_valid = min_val >= rules['min']\n",
    "    max_valid = max_val <= rules['max']\n",
    "    range_valid = min_valid and max_valid\n",
    "    \n",
    "    if not range_valid:\n",
    "        all_ranges_valid = False\n",
    "    \n",
    "    status = \"‚úÖ\" if range_valid else \"‚ùå\"\n",
    "    print(f\"  {column:15} {status} [{min_val:8.1f} - {max_val:8.1f}]\")\n",
    "\n",
    "print(f\"\\nüìä Range validation: {'‚úÖ PASSED' if all_ranges_valid else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate categorical consistency\n",
    "print(\"\\nüè∑Ô∏è CATEGORICAL VALIDATION:\")\n",
    "\n",
    "expected_property_types = {'Single Family', 'Townhouse', 'Condo', 'Duplex', 'Other'}\n",
    "actual_property_types = set(housing_clean['property_type'].unique())\n",
    "property_type_valid = actual_property_types.issubset(expected_property_types)\n",
    "\n",
    "expected_neighborhoods = {'Downtown', 'Suburban', 'Riverside', 'Historic District', 'New Development', 'Other'}\n",
    "actual_neighborhoods = set(housing_clean['neighborhood'].unique())\n",
    "neighborhood_valid = actual_neighborhoods.issubset(expected_neighborhoods)\n",
    "\n",
    "print(f\"  Property types: {'‚úÖ' if property_type_valid else '‚ùå'} {actual_property_types}\")\n",
    "print(f\"  Neighborhoods: {'‚úÖ' if neighborhood_valid else '‚ùå'} {actual_neighborhoods}\")\n",
    "\n",
    "categorical_valid = property_type_valid and neighborhood_valid\n",
    "\n",
    "# Validate logical relationships\n",
    "print(\"\\nüîó LOGICAL RELATIONSHIP VALIDATION:\")\n",
    "\n",
    "reasonable_bathroom_ratio = ((housing_clean['bathrooms'] / housing_clean['bedrooms']) <= 2).all()\n",
    "housing_clean['price_per_sqft'] = housing_clean['price'] / housing_clean['square_feet']\n",
    "reasonable_price_per_sqft = ((housing_clean['price_per_sqft'] >= 50) & \n",
    "                           (housing_clean['price_per_sqft'] <= 600)).all()\n",
    "size_price_correlation = housing_clean['square_feet'].corr(housing_clean['price'])\n",
    "reasonable_correlation = size_price_correlation > 0.5\n",
    "unique_ids = housing_clean['property_id'].nunique() == len(housing_clean)\n",
    "\n",
    "print(f\"  Bathroom/bedroom ratios: {'‚úÖ' if reasonable_bathroom_ratio else '‚ùå'}\")\n",
    "print(f\"  Price per sqft range: {'‚úÖ' if reasonable_price_per_sqft else '‚ùå'}\")\n",
    "print(f\"  Size-price correlation: {'‚úÖ' if reasonable_correlation else '‚ùå'} (r = {size_price_correlation:.3f})\")\n",
    "print(f\"  Unique property IDs: {'‚úÖ' if unique_ids else '‚ùå'}\")\n",
    "\n",
    "logical_valid = reasonable_bathroom_ratio and reasonable_price_per_sqft and reasonable_correlation and unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final quality score\n",
    "quality_checks = [\n",
    "    remaining_missing == 0,\n",
    "    remaining_duplicates == 0,\n",
    "    all_ranges_valid,\n",
    "    categorical_valid,\n",
    "    logical_valid\n",
    "]\n",
    "\n",
    "quality_score = sum(quality_checks)\n",
    "total_checks = len(quality_checks)\n",
    "\n",
    "print(f\"\\nüéØ FINAL DATA QUALITY SCORE: {quality_score}/{total_checks} ({(quality_score/total_checks)*100:.0f}%)\")\n",
    "\n",
    "if quality_score == total_checks:\n",
    "    print(\"üéâ EXCELLENT! Dataset passed all quality checks.\")\n",
    "    print(\"   ‚úÖ Data is ready for analysis and modeling.\")\n",
    "elif quality_score >= total_checks * 0.8:\n",
    "    print(\"üëç GOOD! Dataset passed most quality checks.\")\n",
    "    print(\"   ‚ö†Ô∏è Minor issues may need attention.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING! Significant quality issues remain.\")\n",
    "\n",
    "# Display key statistics\n",
    "print(f\"\\nüìà CLEANED DATASET STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total properties: {len(housing_clean):,}\")\n",
    "print(f\"   ‚Ä¢ Average price: ${housing_clean['price'].mean():,.0f}\")\n",
    "print(f\"   ‚Ä¢ Price range: ${housing_clean['price'].min():,.0f} - ${housing_clean['price'].max():,.0f}\")\n",
    "print(f\"   ‚Ä¢ Average size: {housing_clean['square_feet'].mean():.0f} sq ft\")\n",
    "print(f\"   ‚Ä¢ Most common type: {housing_clean['property_type'].mode()[0]}\")\n",
    "print(f\"   ‚Ä¢ Most common neighborhood: {housing_clean['neighborhood'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.1:** Design additional validation checks specific to real estate data or other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your validation enhancement practice here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Documenting the Cleaning Process\n",
    "\n",
    "Clear documentation ensures others can understand, reproduce, and trust your work. Document what was done, why it was done, the impact, and any limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA CLEANING DOCUMENTATION ===\")\n",
    "\n",
    "current_time = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(\"üìã DATA CLEANING REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Date: {current_time}\")\n",
    "print(f\"Dataset: Housing Market Data\")\n",
    "print(f\"Purpose: Prepare housing data for market analysis\")\n",
    "\n",
    "print(f\"\\nüìä TRANSFORMATION SUMMARY:\")\n",
    "print(f\"‚îú‚îÄ Original: {housing_data.shape[0]:,} properties, {housing_data.shape[1]} features\")\n",
    "print(f\"‚îú‚îÄ Final: {housing_clean.shape[0]:,} properties, {housing_clean.shape[1]} features\")\n",
    "print(f\"‚îú‚îÄ Retention: {(len(housing_clean) / len(housing_data)) * 100:.1f}%\")\n",
    "print(f\"‚îî‚îÄ Removed: {len(housing_data) - len(housing_clean):,} records\")\n",
    "\n",
    "# Calculate improvement metrics\n",
    "original_missing = housing_data.isnull().sum().sum()\n",
    "final_missing = housing_clean.isnull().sum().sum()\n",
    "original_duplicates = housing_data.duplicated().sum()\n",
    "final_duplicates = housing_clean.duplicated().sum()\n",
    "original_prop_types = housing_data['property_type'].nunique()\n",
    "final_prop_types = housing_clean['property_type'].nunique()\n",
    "\n",
    "print(f\"\\nüîß CLEANING ACTIONS PERFORMED:\")\n",
    "print(f\"\\n1Ô∏è‚É£ TEXT STANDARDIZATION:\")\n",
    "print(f\"   ‚Ä¢ Property types: {original_prop_types} ‚Üí {final_prop_types} categories\")\n",
    "print(f\"   ‚Ä¢ Neighborhoods: Standardized to consistent format\")\n",
    "print(f\"   ‚Ä¢ Property IDs: Converted to numeric format\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ MISSING VALUE TREATMENT:\")\n",
    "print(f\"   ‚Ä¢ Total missing: {original_missing} ‚Üí {final_missing}\")\n",
    "print(f\"   ‚Ä¢ Ages: Imputed using neighborhood medians\")\n",
    "print(f\"   ‚Ä¢ Bathrooms: Imputed using bedroom/type patterns\")\n",
    "print(f\"   ‚Ä¢ Prices: Imputed using similar properties\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ QUALITY IMPROVEMENTS:\")\n",
    "print(f\"   ‚Ä¢ Duplicates removed: {original_duplicates} ‚Üí {final_duplicates}\")\n",
    "print(f\"   ‚Ä¢ Impossible values corrected\")\n",
    "print(f\"   ‚Ä¢ Extreme outliers investigated and corrected\")\n",
    "print(f\"   ‚Ä¢ Final quality score: {quality_score}/{total_checks} ({(quality_score/total_checks)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ BUSINESS IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Dataset ready for market analysis and modeling\")\n",
    "print(f\"   ‚Ä¢ Consistent categories enable accurate comparisons\")\n",
    "print(f\"   ‚Ä¢ Clean price data suitable for valuation models\")\n",
    "print(f\"   ‚Ä¢ No missing values to interfere with algorithms\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è LIMITATIONS:\")\n",
    "print(f\"   ‚Ä¢ Imputed values may introduce slight bias\")\n",
    "print(f\"   ‚Ä¢ Outlier corrections based on statistical rules\")\n",
    "print(f\"   ‚Ä¢ Synthetic dataset may not capture all real-world complexity\")\n",
    "\n",
    "print(f\"\\nüìã RECOMMENDATIONS:\")\n",
    "print(f\"   1. Use cleaned dataset for market analysis\")\n",
    "print(f\"   2. Consider flagging imputed values in sensitive analyses\")\n",
    "print(f\"   3. Validate results against domain expertise\")\n",
    "print(f\"   4. Monitor data quality in future updates\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   üìä {housing_clean.shape[0]:,} clean properties ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You‚Äôve completed a realistic data cleaning lab, tackling the kinds of messy data challenges you‚Äôll face in real projects.\n",
    "\n",
    "### Key Skills Practiced\n",
    "\n",
    "- **Systematic assessment:** You learned to identify, categorize, and prioritize data quality issues before cleaning.\n",
    "- **Text standardization:** You created functions to clean up inconsistent categorical data, a common real-world problem.\n",
    "- **Missing value imputation:** You used context-aware strategies, applying domain knowledge and data patterns to fill gaps.\n",
    "- **Outlier handling:** You practiced distinguishing between errors and legitimate extreme values, using both statistics and business logic.\n",
    "- **Validation and documentation:** You validated your results and documented your process for transparency and reproducibility.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Most of your time as a data scientist will be spent preparing and cleaning data. Clean data is essential for trustworthy analysis and modeling‚Äîwithout it, results are unreliable.\n",
    "\n",
    "### Real-World Relevance\n",
    "\n",
    "These techniques apply across industries: finance, healthcare, retail, manufacturing, and more. The skills you‚Äôve built are foundational for any data-driven role.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always work on a copy of your data.\n",
    "- Document every cleaning decision.\n",
    "- Combine statistical methods with domain knowledge.\n",
    "- Validate your results from multiple angles.\n",
    "- Be transparent about limitations and assumptions.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "You‚Äôre now equipped to approach any data cleaning task methodically: assess, plan, clean, validate, and document. Continue practicing on new datasets, try advanced imputation, and always seek to understand the root causes of data issues.\n",
    "\n",
    "Clean data is the foundation of all good data science‚Äîyour work here sets you up for success in any analysis or modeling project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
